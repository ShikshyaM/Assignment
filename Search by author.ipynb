{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tkinter import messagebox\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawler Component\n",
    "URL = \"https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/\"\n",
    "profile_url = \"https://pureportal.coventry.ac.uk/en/persons/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maximum_page():\n",
    "    first = requests.get(URL)\n",
    "    soup = BeautifulSoup(first.text, 'html.parser')\n",
    "    final_page = soup.select('#main-content > div > section > nav > ul > li:nth-child(12) > a')[0]['href']\n",
    "    fp = final_page.split('=')[-1]\n",
    "    return int(fp)\n",
    "mx = get_maximum_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_department(researcher):\n",
    "    \n",
    "    l1 = researcher.find('div', class_='rendering_person_short')\n",
    "      \n",
    "    for span in l1.find_all('span'):\n",
    "        # Check department\n",
    "        if span.text == str('Centre for Intelligent Healthcare'):\n",
    "            name = researcher.find('h3', class_='title').find('span').text\n",
    "            \n",
    "            return name\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def remove_data_from_csv(file_path):\n",
    "    # Read the CSV file and keep only the header row\n",
    "    df = pd.read_csv(file_path, nrows=0)\n",
    "\n",
    "    # Save the DataFrame with only the header row back to the CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n",
    "def update_csv(database):\n",
    "    current_data = pd.read_csv(database)\n",
    "    return current_data        \n",
    "\n",
    "def enter_each_researchers_publication(researcher, url, df):\n",
    "    current_data = pd.read_csv(df)  # Load existing DataFrame from the CSV file\n",
    "\n",
    "    new_url = url + str(researcher).replace(' ', '-').lower() + '/publications/'\n",
    "    page = requests.get(new_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(id=\"main-content\")\n",
    "    papers = results.find_all(\"li\", class_=\"list-result-item\")\n",
    "\n",
    "    new_rows = []  # List to store the new rows\n",
    "\n",
    "    for index, paper in enumerate(papers, start=len(current_data)+1):\n",
    "        title_element = paper.find('h3', class_='title')\n",
    "        title = title_element.find('span').text if title_element else None\n",
    "\n",
    "        author_element = paper.find('a', class_='link person')\n",
    "        author = author_element.find('span').text if author_element else None\n",
    "\n",
    "        date_element = paper.find('span', class_=\"date\")\n",
    "        date = date_element.text if date_element else None\n",
    "\n",
    "        link_element = paper.find('h3', class_='title').find('a', href=True)\n",
    "        link = link_element['href'] if link_element else None\n",
    "\n",
    "        new_row = {'SN': index,\n",
    "                   'Title': title,\n",
    "                   'Author': author,\n",
    "                   'Published': date,\n",
    "                   'Link': link}\n",
    "\n",
    "        new_rows.append(new_row)  # Append the new row to the list\n",
    "\n",
    "    new_data = pd.DataFrame(new_rows)  # Create a new DataFrame with the new rows\n",
    "    updated_data = pd.concat([current_data, new_data], ignore_index=True)  # Concatenate the existing DataFrame with the new DataFrame\n",
    "\n",
    "    updated_data.to_csv(df, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(mx):\n",
    "    df = r'database2.csv'\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i > 45:\n",
    "            break\n",
    "\n",
    "        if i > 0:\n",
    "            url = URL + '?page=' + str(i)\n",
    "        else:\n",
    "            url = URL\n",
    "\n",
    "        i = i + 1\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        results = soup.find(id=\"main-content\")\n",
    "        researchers = results.find_all(\"li\", class_=\"grid-result-item\")\n",
    "\n",
    "        for researcher in researchers:\n",
    "            check = researcher.find('div', class_='stacked-trend-widget')\n",
    "            if check:\n",
    "                name = check_department(researcher)\n",
    "                if name is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    enter_each_researchers_publication(name, profile_url, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_csv()\n",
    "remove_data_from_csv(r'database2.csv')\n",
    "update_csv(database=r'database2.csv')\n",
    "scrape(mx)\n",
    "\n",
    "# Indexing Component\n",
    "scraped_db = pd.read_csv('database2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    processed_text = \" \".join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index():\n",
    "    index = {}\n",
    "    for i, row in scraped_db.iterrows():\n",
    "        title = row['Title']\n",
    "        author = row['Author']\n",
    "        processed_title = preprocess_text(title)\n",
    "        processed_author = preprocess_text(author)\n",
    "        \n",
    "        # Update index with title\n",
    "        if processed_title not in index:\n",
    "            index[processed_title] = []\n",
    "        index[processed_title].append(i)\n",
    "        \n",
    "        # Update index with author\n",
    "        if processed_author not in index:\n",
    "            index[processed_author] = []\n",
    "        index[processed_author].append(i)\n",
    "    \n",
    "    with open('index.json', 'w') as f:\n",
    "        json.dump(index, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the index from the JSON file\n",
    "with open('index.json', 'r') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "def search_publications():\n",
    "    query = entry.get(1.0, \"end\")\n",
    "    processed_query = preprocess_text(query)\n",
    "    \n",
    "    if processed_query in index:\n",
    "        matching_indices = index[processed_query]\n",
    "        matching_publications = scraped_db.loc[matching_indices]\n",
    "        \n",
    "        # True positives: Matching publications retrieved and relevant\n",
    "        true_positives = len(matching_publications)\n",
    "        \n",
    "        # False negatives: Relevant publications not retrieved\n",
    "        false_negatives = len(scraped_db) - true_positives\n",
    "        \n",
    "        # False positives: Non-relevant publications retrieved\n",
    "        false_positives = 0\n",
    "        \n",
    "        # True negatives: Non-relevant publications not retrieved\n",
    "        true_negatives = 0\n",
    "        \n",
    "        # Calculate false positives and true negatives\n",
    "        for i, row in scraped_db.iterrows():\n",
    "            if i not in matching_indices:\n",
    "                false_positives += 1\n",
    "            else:\n",
    "                true_negatives += 1\n",
    "        \n",
    "        # Calculate the predicted and actual labels\n",
    "        predicted_labels = [1] * true_positives + [0] * false_positives\n",
    "        actual_labels = [1] * (true_positives + false_negatives) + [0] * (false_positives + true_negatives)\n",
    "        \n",
    "        # Adjust the length of the lists if they are different\n",
    "        max_length = max(len(predicted_labels), len(actual_labels))\n",
    "        predicted_labels += [0] * (max_length - len(predicted_labels))\n",
    "        actual_labels += [0] * (max_length - len(actual_labels))\n",
    "        \n",
    "        # Calculate the confusion matrix\n",
    "        cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "        \n",
    "        # Display the confusion matrix\n",
    "        messagebox.showinfo(\"Confusion Matrix\", str(cm))\n",
    "        \n",
    "        # Display the matching publications\n",
    "        messagebox.showinfo(\"Search Results\", matching_publications.to_string())\n",
    "    else:\n",
    "        messagebox.showinfo(\"Search Results\", \"No matching publications found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main window\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "window = tk.Tk()\n",
    "window.title(\"Coventry University-Publication Search\")\n",
    "window.geometry(\"600x650\")\n",
    "window.configure(bg=\"#919492\")\n",
    "window.resizable(0,0)\n",
    "from PIL import ImageTk, Image\n",
    "img=(Image.open(\"coventry-university-logo.png\"))\n",
    "imgg=img.resize((200,200))\n",
    "imggg=ImageTk.PhotoImage(imgg)\n",
    "\n",
    "lbl=Label(window,image=imggg,bg=\"#919492\")\n",
    "lbl.pack(side=TOP)\n",
    "# Create and position the search label\n",
    "label = tk.Label(window, text=\"Enter author name or title name:\",fg=\"white\",font=(\"Arial\", 20,\"italic\"),bg=\"#919492\")\n",
    "label.pack(pady=10)\n",
    "\n",
    "# Create and position the search entry\n",
    "entry = tk.Text(window,height=10,width=30,font=(\"Arial\", 20))\n",
    "entry.pack()\n",
    "\n",
    "# Create and position the search button\n",
    "button = tk.Button(window, text=\"Search\",font=(\"Arial\", 20),bg=\"#10164B\",fg=\"white\",command=search_publications)\n",
    "button.pack(pady=10)\n",
    "\n",
    "\n",
    "# Start the main event loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
